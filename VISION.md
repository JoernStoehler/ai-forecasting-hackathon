# Vision

## Product Goal (written by Jörn)
- Serious policy simulation game where the player alternates turns with an LLM-based forecaster that consults expert-curated and expert-written background material and instructions for the game; static Vite/React SPA, client-only storage, deployable to AI Studio Build for wide audience access.

## What This File Is (and isn’t)
- This is a stable “north star” for the product: what we’re building, for whom, and what constraints matter.
- This is **not** authoritative planning. The source of truth for work items is GitHub Issues (and Jörn assigns issues to agents once he reviewed issues and decides they are now ready for implementation).

## Core Player Experience (north star)
- The player starts a new game, with opt-out low-overhead tutorial messages for first-timers, e.g. to explain mechanics and UI and mayyybe the most important domain knowledge.
- The player reads through the presented scenario state and history, thinks, and makes decisions.
- The player makes decisions via the UI, usually all reversible and commutative until the turn is done (e.g. editable text box that is only submitted when the player clicks "Send", or decision toggles/elements that can be adjusted any time).
- The player ends their turn, and decisions become locked.
- The LLM-based forecaster processes the player's turn, and updates the scenario state and history. Breaking down the forecaster step further:
  - The LLM that backs the forecaster streams receives a generated prompt, and streams back text output. Potentially the output adheres to a json schema we can specify, and we get chunks of text back. Or freeform text, if we dislike how chunking works, how schemas work etc.
  - There's some time-to-first-token latency, and afterwards a rather constant rate of tokens per second until the last token. Think of tokens as sth like word fragments.
  - We can process the output as it streams in, processing it into semantically meaningful commands that express the intent of the LLM. So we want the LLM to basically return a sequence of commands as the forecaster's full turn.
  - We can execute the commands, using some very simple flexible scenario engine. Most logic is done in the mind of the forecaster, e.g. the LLM can also turn player Commands (a text blurb, a button press, etc.) into scenario engine commands. The scenario engine is more like a board with figures, where in principle players could move pieces illegally, change the rules on the fly etc. It's up to the forecaster to read the background materials and instructions in order to deliver an actually useful policy simulation scenario.
- The UI updates as the commands get executed, with some extra ui state ofc like position, animations, etc.
- The overall flow we have is
  - Player uses the UI to learn about the scenario state (e.g. reading events, scrolling back in time, looking at diagrams) and thinks while staring at the screen
  - While doing so the player can also designate commands/intent, adjust the designated commands/intent, and finally submit them as his turn
  - The LLM then plans what happens, i.e. it translated player intent and its own freedom as game master / forecaster into lower-level scenario engine commands
  - The scenario engine commands affect the saved scenario state and the UI reflects the changes as they come in, adding meta state e.g. which changes to the UI require animations, etc.
- A lot of player intent can be rather implicit, e.g. we can record that the player opened a news article for the first time. We don't want to spam the LLM, so we filter a lot while writing the prompt that we send to the LLM api (e.g. we may only care about whether an article was ever opened, not when or how many times)
- In general, we use the React pattern:
  - UI is a pure function of state
  - State is reduced by commands (sometimes called actions)
  - Commands are generated by player actions and the LLM forecaster
- We record the engine state using an event sourcing pattern, i.e. just dump all commands into a log, and reduce to get the state. We don't mind spamming a lot of UI effects into the log files, since the LLM prompt is filtered and optimized for LLM consumption anyway later.
  - The LLM prompt is a function of state
  - The LLM output is a non-deterministic function of the prompt, and a token/text stream
  - The LLM commands are a function of the LLM output, in a streaming fashion (i.e. tokens => commands)
  - The Player UI is a function of state
  - The Player output is a non-deterministic function of the UI (via thinking + clicking)
  - The Player commands are a function of the Player output (turning UI interactions into actual commands we want to record and reduce state with)
  - Optional: we can either treat state as a pure function that reduces the sequence of commands, and we record all commands, or we distinguish between ephemeral state (e.g. UI in React sometimes has a lot of that) and reproducible state (semantic content of the scenario and all we count towards it, i.e. maybe the list of opened reports so far is semantic information about the policy wargame). Reproducible state is a reduced function of events, where events are emitted by commands. Ephemeral state is a reduced function of events as well, again emitted by commands, but potentially a different set of events. I.e., commands emit two kinds of events, that get reduced into two states, where the reproducible state logs the events, while the ephemeral state doesn't log them but just applies them and forgets them. This way we conceputally don't bloat the event log with every pixel movement of the mouse cursor.
  - Potentially, the ephemereal state is entirely under the hood, i.e. we just let the browser handle it, and everything we actually code interacts with the reproducible state and event log only. In that case, we may even want commands to emit a single event only.
- The main policy wargame phases are
  - an initial setup (new game)
  - alternating turns between player and LLM forecaster
  - post-game screen (triggered when the LLM forecaster declares the end of the scenario)
- The post-game screen features an analysis written by the LLM forecaster, now consulting extra material, and allows to see previously hidden notes and secret scenario knowledge from the forecaster, e.g. what the other nation states were up to, or what latent variables were rolled behind the scenes. The post-game screen is interactive, i.e. questions can be asked to the LLM forecaster, in a typical LLM-chat manner. The final result can be copy-pasted to the clipboard for easy sharing (e.g. for twitter "I completed the Cuban Missile Crisis 1962 scenario. Nuclear war destroyed both sides.").

- Above was like the overall high-levle perspective on how to do any policy game, nothing focused on ai x-risk or on forecasting a out-of-distribution future yet
- In fact, that's intended! We mostly write a general engine and app and then all the domain logic lives in the form of materials provided to the LLM forecaster. Again the analogy: we manifacture here a table with figurines and a paintable map and an empty notepad to write in, and the LLM forecaster handles these masterials, using their knowledge and a rulebook they have read, as well as what the player says out loud or indicates via pointing at things on the table.
- We then provide
  - A simple background material bundle about some toy domain, for testing and demoing and prototyping and easier comparisons; faster to play, less complex, clearer feedback on how good the LLM forecaster is at their tasks (e.g. rule compliance, realism, fun). possibly in the past, about a real event that could've gone differently, or possibly in the future, about some simpler topic such as climate change policy while ignoring the existence of AI entirely.
  - A complex background material bundle about ai x-risk forecasting, which is a super hard domain bc it is so different from the past, involves *alien minds with alien preferences and alien skill-set imbalances*, and is faster developing (exponential progress with 6 month doubling times) compared to climate change or even breakthrough technologies like nuclear weapons or the internet or industrialization.

- Replayability comes from the non-determinism of the LLM forecaster, as well as injected randomness (via a prng that the engine manages, i.e. the state contains the prng state and the last rolled value, and some actions then progress the prng and replace the last rolled value with a new one). The LLM forecaster then uses the prng, e.g. to decide how much progress happens on AI capabilities the next 6 months or whether a particular diplomatic negotiation attempt succeeds or fails. Basically: the game materials include a bunch of dice the game master can roll in secret and then interpret however they like.
- We also provide some inital randomness, e.g. by having multiple material bundles (or rather, variations that pick one of several replacement text snippets). This way, we can provide more customized materials to the AI forecaster, e.g. "in this game, we use a model of China-US relation dynamics by Schmidt et al 2020, see attached summary of the central motives of China-US diplomacy".
- We could do branching timelines, but honestly, feels boring. Better to have players explore a highly uncertain scenario landscape with initial randomness + calibrated unpredictability (i.e. the forecasting AI will have dice to better balance how surprising its predictions are bc LLMs sometimes have positivity bias, or lean towards predictable median cases, or fall for typical reasoning biases like conjunction fallacy, UNLESS we prompt them to roll dice and think quantitatively about what kind of outcome to produce).
- Since we can only inject randomness into the prompts, I suggest that we basically use a Command for turn change that setups the PRNG, e.g. the prompt always contains a line "After the next 6 months, total AI capability growth has fallen into the 76th percentile (in standard deviations: +0.7) compared to what expectations a calibrated expert forecaster has at the start of the 6 month interval. Formulate your forecast, i.e. interpret the percentile, then flesh it out into a full 6 month timeline that you communicate as Commands[] output."

- Deployment happens via AI Studio Build, so that anyone can play the game in their browser without installing anything, and importantly, anyone has access to some free LLM api budget without having to configure any api keys or payment methods, and without incurring costs for the developer team.
- We share the link to the AI Studio Build app on social media etc.
- Players can download and upload save files, e.g. to transfer between devices. MAYBE we add google drive or sth as well. MAYBE we add a telemetry server that stores savefiles, but imo that's quite a bit of thinkign we need to do before deciding to do that, there may be financial, legal consequences ehre, though at least the data isn't sensitive imo so we can even run massive data analysis on our telemetry server about the recorded games. We just don't fingerprint people, and all should be good. And ofc an opt-out toggle that is displayed at the start or sth.

- Wrt settings: dark+light mode, mayybe audio toggle, mayybe font size toggle; mostly we just do the good ergonomic defaults and don't overcomplicate things, and stick to what people are used to from the normal web. The target audience of this game isn't just tech people, but anyone interested in policy and ai x-risk.

## Hard Constraints / Defaults
- Static web app first (Vite/React SPA); no mandatory backend.
- Client-only storage by default (local-first); sharing happens via import/export unless explicitly expanded.
- Deployable to AI Studio Build for wide audience access.
- “Serious policy sim” tone: optimize for clarity, interpretability, and consistent rules over novelty.

## Progress Checklist (non-authoritative snapshot)
- [x] Playable single-page timeline editor (create/edit events)
- [x] Local persistence + import/export
- [x] LLM forecaster call on submit (Gemini integration)
- [ ] Deterministic replay mode (cassette) for dev + E2E
- [ ] Materials pack selection + obvious grounding in UI/CLI
- [ ] Minimal regression tests for the core loop (smoke/E2E)
- [ ] Clear stance + UX for “branching timelines” vs “post-mortem reveal”

## How To Propose Work
- Write proposals as GitHub Issues (use `.github/ISSUE_TEMPLATE/task.md` as the default structure).
- Keep this file focused on stable intent and constraints; link to Issues instead of duplicating task breakdowns here.
